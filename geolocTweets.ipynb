{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "csv.QUOTE_NONE\n",
    "# data = pd.read_csv('dev_tweets.txt', header = None)\n",
    "df = pd.read_csv('train_tweets.txt', sep=\"\\n\",encoding='latin-1', header=None, quoting=csv.QUOTE_NONE)\n",
    "dfPred = pd.read_csv('test_tweets.txt', sep=\"\\n\",encoding='latin-1', header=None, quoting=csv.QUOTE_NONE)\n",
    "# data = pd.read_csv('dev_tweets.txt', sep=\",\", header=None)\n",
    "df.columns = [\"row\"]\n",
    "dfPred.columns = [\"row\"]\n",
    "# print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dfPred = dfPred.pop('row').str.split(',')\n",
    "dfPred['tweet_id'], dfPred['user_tweet'], dfPred['loc'] = tmp_dfPred.str[0], tmp_dfPred.str[1:-1].str.join(sep=','), tmp_dfPred.str[-1]\n",
    "\n",
    "# print(dfPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = data.pop('row').str.split(',', expand=True)\n",
    "# v.columns = ['tweet_id', 'user_tweet', 'loc']\n",
    "# data[['tweet_id','user_tweet']] = data.pop('row').str.split(',', n=1, expand=True)\n",
    "# data[['user_tweet','loc']] = data['user_tweet'].str.rsplit(',', n=1, expand=True)\n",
    "# print (data)\n",
    "\n",
    "tmp_df = df.pop('row').str.split(',')\n",
    "df['tweet_id'], df['user_tweet'], df['loc'] = tmp_df.str[0], tmp_df.str[1:-1].str.join(sep=','), tmp_df.str[-1]\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_id'], df['tweet_text'] = df['user_tweet'].str.split(',', 1).str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPred['user_id'], dfPred['tweet_text'] = dfPred['user_tweet'].str.split(',', 1).str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['tweet_id','user_id','tweet_text','loc','user_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPred = dfPred[['tweet_id','user_id','tweet_text','loc','user_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('user_tweet', 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPred = dfPred.drop('user_tweet', 1)\n",
    "dfPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweet_csv.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPred.to_csv('tweet_csv.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re as regex\n",
    "import numpy as np\n",
    "import plotly\n",
    "from plotly import graph_objs\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ny = len(df[df[\"loc\"] == \"NewYork\"])\n",
    "ga = len(df[df[\"loc\"] == \"Georgia\"])\n",
    "ca = len(df[df[\"loc\"] == \"California\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ny)\n",
    "print(ga)\n",
    "print(ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = [\n",
    "    graph_objs.Bar(\n",
    "        x=[\"NewYork\",\"Georgia\",\"California\"],\n",
    "        y=[ny, ga, ca],\n",
    ")]\n",
    "plotly.offline.iplot({\"data\":dist, \"layout\":graph_objs.Layout(title=\"Sentiment type distribution in training set\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweet_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPred[\"tweet_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_by_lambda(expression, word_array):\n",
    "    return len(list(filter(expression, word_array)))\n",
    "\n",
    "\n",
    "def count_occurences(character, word_array):\n",
    "    counter = 0\n",
    "    for (j, word) in enumerate(word_array):\n",
    "        for char in word:\n",
    "            if char == character:\n",
    "                counter += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "def count_by_regex(regex, plain_text):\n",
    "    return len(regex.findall(plain_text))\n",
    "\n",
    "\n",
    "def addNewColumns():\n",
    "    add_column(df, 'splitted_text', map(lambda txt: txt.split(' '),\n",
    "               df['tweet_text']))\n",
    "\n",
    "            # number of uppercase words\n",
    "\n",
    "    uppercase = list(map(lambda txt: count_by_lambda(lambda word: word \\\n",
    "                     == word.upper(), txt), df['splitted_text']))\n",
    "    add_column(df, 'number_of_uppercase', uppercase)\n",
    "\n",
    "            # number of !\n",
    "\n",
    "    exclamations = list(map(lambda txt: count_occurences('!', txt),\n",
    "                        df['splitted_text']))\n",
    "    add_column(df, 'number_of_exclamation', exclamations)\n",
    "\n",
    "            # number of ?\n",
    "\n",
    "    questions = list(map(lambda txt: count_occurences('?', txt),\n",
    "                     df['splitted_text']))\n",
    "    add_column(df, 'number_of_question', questions)\n",
    "\n",
    "            # number of ...\n",
    "\n",
    "    ellipsis = list(map(lambda txt: \\\n",
    "                    count_by_regex(regex.compile(r\"\\.\\s?\\.\\s?\\.\"),\n",
    "                    txt), df['tweet_text']))\n",
    "    add_column(df, 'number_of_ellipsis', ellipsis)\n",
    "\n",
    "            # number of hashtags\n",
    "\n",
    "    hashtags = list(map(lambda txt: count_occurences('#', txt),\n",
    "                    df['splitted_text']))\n",
    "    add_column(df, 'number_of_hashtags', hashtags)\n",
    "\n",
    "            # number of mentions\n",
    "\n",
    "    mentions = list(map(lambda txt: count_occurences('@', txt),\n",
    "                    df['splitted_text']))\n",
    "    add_column(df, 'number_of_mentions', mentions)\n",
    "\n",
    "            # number of quotes\n",
    "\n",
    "    quotes = list(map(lambda plain_text: int(count_occurences(\"'\",\n",
    "                  [plain_text.strip(\"'\").strip('\"')]) / 2\n",
    "                  + count_occurences('\"', [plain_text.strip(\"'\"\n",
    "                  ).strip('\"')]) / 2), df['tweet_text']))\n",
    "    add_column(df, 'number_of_quotes', quotes)\n",
    "\n",
    "            # number of urls\n",
    "\n",
    "    urls = list(map(lambda txt: \\\n",
    "                count_by_regex(regex.compile(r\"http.?://[^\\s]+[\\s]?\"),\n",
    "                txt), df['tweet_text']))\n",
    "    add_column(df, 'number_of_urls', urls)\n",
    "\n",
    "\n",
    "def add_column(df, column_name, column_content):\n",
    "    df.loc[:, column_name] = pd.Series(column_content, index=df.index)\n",
    "\n",
    "\n",
    "addNewColumns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_by_lambda(expression, word_array):\n",
    "    return len(list(filter(expression, word_array)))\n",
    "\n",
    "\n",
    "def count_occurences(character, word_array):\n",
    "    counter = 0\n",
    "    for (j, word) in enumerate(word_array):\n",
    "        for char in word:\n",
    "            if char == character:\n",
    "                counter += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "def count_by_regex(regex, plain_text):\n",
    "    return len(regex.findall(plain_text))\n",
    "\n",
    "\n",
    "def addNewColumns():\n",
    "    add_column(dfPred, 'splitted_text', map(lambda txt: txt.split(' '),\n",
    "               dfPred['tweet_text']))\n",
    "\n",
    "            # number of uppercase words\n",
    "\n",
    "    uppercase = list(map(lambda txt: count_by_lambda(lambda word: word \\\n",
    "                     == word.upper(), txt), dfPred['splitted_text']))\n",
    "    add_column(dfPred, 'number_of_uppercase', uppercase)\n",
    "\n",
    "            # number of !\n",
    "\n",
    "    exclamations = list(map(lambda txt: count_occurences('!', txt),\n",
    "                        dfPred['splitted_text']))\n",
    "    add_column(dfPred, 'number_of_exclamation', exclamations)\n",
    "\n",
    "            # number of ?\n",
    "\n",
    "    questions = list(map(lambda txt: count_occurences('?', txt),\n",
    "                     dfPred['splitted_text']))\n",
    "    add_column(dfPred, 'number_of_question', questions)\n",
    "\n",
    "            # number of ...\n",
    "\n",
    "    ellipsis = list(map(lambda txt: \\\n",
    "                    count_by_regex(regex.compile(r\"\\.\\s?\\.\\s?\\.\"),\n",
    "                    txt), dfPred['tweet_text']))\n",
    "    add_column(dfPred, 'number_of_ellipsis', ellipsis)\n",
    "\n",
    "            # number of hashtags\n",
    "\n",
    "    hashtags = list(map(lambda txt: count_occurences('#', txt),\n",
    "                    dfPred['splitted_text']))\n",
    "    add_column(dfPred, 'number_of_hashtags', hashtags)\n",
    "\n",
    "            # number of mentions\n",
    "\n",
    "    mentions = list(map(lambda txt: count_occurences('@', txt),\n",
    "                    dfPred['splitted_text']))\n",
    "    add_column(dfPred, 'number_of_mentions', mentions)\n",
    "\n",
    "            # number of quotes\n",
    "\n",
    "    quotes = list(map(lambda plain_text: int(count_occurences(\"'\",\n",
    "                  [plain_text.strip(\"'\").strip('\"')]) / 2\n",
    "                  + count_occurences('\"', [plain_text.strip(\"'\"\n",
    "                  ).strip('\"')]) / 2), dfPred['tweet_text']))\n",
    "    add_column(dfPred, 'number_of_quotes', quotes)\n",
    "\n",
    "            # number of urls\n",
    "\n",
    "    urls = list(map(lambda txt: \\\n",
    "                count_by_regex(regex.compile(r\"http.?://[^\\s]+[\\s]?\"),\n",
    "                txt), dfPred['tweet_text']))\n",
    "    add_column(dfPred, 'number_of_urls', urls)\n",
    "\n",
    "\n",
    "def add_column(dfPred, column_name, column_content):\n",
    "    dfPred.loc[:, column_name] = pd.Series(column_content, index=dfPred.index)\n",
    "\n",
    "\n",
    "addNewColumns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfTrain.to_csv('trainExtraColumns.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfTest.to_csv('testExtraColumns.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_by_regex(reg,tweets):\n",
    "    tweets = regex.sub(reg,'',tweets)\n",
    "    return tweets\n",
    "def remove_usernames(tweets):\n",
    "    return remove_by_regex('@[^\\s]+[\\s]?', tweets)\n",
    "def remove_urls(tweets):\n",
    "    return remove_by_regex('http.?://[^\\s]+[\\s]?', tweets)\n",
    "def remove_numbers(tweets):\n",
    "    return remove_by_regex('\\s?[0-9]+\\.?[0-9]*', tweets)\n",
    "def remove_special_chars(tweets):\n",
    "    return remove_by_regex('\\\\|\\?|\\.|\\!|\\/|\\;|\\:|$|#|%|&|,|\\'|]|\\|-|\\\"|_|\\[|\\||{|', tweets)\n",
    "def remove_otherspecial_chars(tweets):\n",
    "    return remove_by_regex('[-?_)(*@=~]+', tweets)\n",
    "def callAll(tweets):\n",
    "    tweets = remove_usernames(tweets)\n",
    "    tweets = remove_urls(tweets)\n",
    "    tweets = remove_special_chars(tweets)\n",
    "    tweets = remove_otherspecial_chars(tweets)\n",
    "    tweets = remove_numbers(tweets)\n",
    "    return tweets\n",
    "for i, row_value in df['tweet_text'].iteritems():\n",
    "    tweets = df['tweet_text'][i]\n",
    "    df['tweet_text'][i] = callAll(tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_by_regex(reg,tweets):\n",
    "    tweets = regex.sub(reg,'',tweets)\n",
    "    return tweets\n",
    "def remove_usernames(tweets):\n",
    "    return remove_by_regex('@[^\\s]+[\\s]?', tweets)\n",
    "def remove_urls(tweets):\n",
    "    return remove_by_regex('http.?://[^\\s]+[\\s]?', tweets)\n",
    "def remove_numbers(tweets):\n",
    "    return remove_by_regex('\\s?[0-9]+\\.?[0-9]*', tweets)\n",
    "def remove_special_chars(tweets):\n",
    "    return remove_by_regex('\\\\|\\?|\\.|\\!|\\/|\\;|\\:|$|#|%|&|,|\\'|]|\\|-|\\\"|_|\\[|\\||{|', tweets)\n",
    "def remove_otherspecial_chars(tweets):\n",
    "    return remove_by_regex('[-?_)(*@=~]+', tweets)\n",
    "def callAllPred(tweets):\n",
    "    tweets = remove_usernames(tweets)\n",
    "    tweets = remove_urls(tweets)\n",
    "    tweets = remove_special_chars(tweets)\n",
    "    tweets = remove_otherspecial_chars(tweets)\n",
    "    tweets = remove_numbers(tweets)\n",
    "    return tweets\n",
    "for i, row_value in dfPred['tweet_text'].iteritems():\n",
    "    tweets = dfPred['tweet_text'][i]\n",
    "    dfPred['tweet_text'][i] = callAllPred(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_by_regex(tweets):\n",
    "#     tweets = regex.sub('@[^\\s]+[\\s]?','',tweets)\n",
    "#     return tweets\n",
    "# def remove_usernames(tweets):\n",
    "#     return remove_by_regex(tweets)\n",
    "# for i, row_value in df['tweet_text'].iteritems():\n",
    "#     tweets = df['tweet_text'][i]\n",
    "#     df['tweet_text'][i] = regex.sub('@[^\\s]+[\\s]?','',tweets)\n",
    "\n",
    "\n",
    "\n",
    "# def remove_by_regex(reg,tweets):\n",
    "#     tweets = regex.sub(reg,'',tweets)\n",
    "#     return tweets\n",
    "# def remove_usernames(tweets):\n",
    "#     return remove_by_regex('@[^\\s]+[\\s]?', tweets)\n",
    "# for i, row_value in df['tweet_text'].iteritems():\n",
    "#     tweets = df['tweet_text'][i]\n",
    "#     df['tweet_text'][i] = remove_usernames(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row_value in df['tweet_text'].iteritems():\n",
    "        print(df['tweet_text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row_value in dfPred['tweet_text'].iteritems():\n",
    "        print(dfPred['tweet_text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, tokenizer=nltk.word_tokenize):\n",
    "        def tokenize_row(row):\n",
    "            row[\"tweet_text\"] = tokenizer(row[\"tweet_text\"])\n",
    "            row[\"tokenized_text\"] = [] + row[\"tweet_text\"]\n",
    "            return row\n",
    "\n",
    "        df = df.apply(tokenize_row, axis=1)\n",
    "tokenize(df)\n",
    "\n",
    "# for i, row_value in df['tweet_text'].iteritems():\n",
    "#     tweets = df['tweet_text'][i]\n",
    "#     df['tweet_text'][i] = callAll(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dfPred, tokenizer=nltk.word_tokenize):\n",
    "        def tokenize_row(row):\n",
    "            row[\"tweet_text\"] = tokenizer(row[\"tweet_text\"])\n",
    "            row[\"tokenized_text\"] = [] + row[\"tweet_text\"]\n",
    "            return row\n",
    "\n",
    "        dfPred = dfPred.apply(tokenize_row, axis=1)\n",
    "tokenize(dfPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(df, stemmer=nltk.PorterStemmer()):\n",
    "        def stem_and_join(row):\n",
    "            row[\"tweet_text\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"tweet_text\"]))\n",
    "            return row\n",
    "\n",
    "        df = df.apply(stem_and_join, axis=1)\n",
    "stem(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(dfPred, stemmer=nltk.PorterStemmer()):\n",
    "        def stem_and_join(row):\n",
    "            row[\"tweet_text\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"tweet_text\"]))\n",
    "            return row\n",
    "\n",
    "        dfPred = dfPred.apply(stem_and_join, axis=1)\n",
    "stem(dfPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter()\n",
    "for idx in df.index:\n",
    "    words.update(df.loc[idx, \"tweet_text\"])\n",
    "\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=nltk.corpus.stopwords.words(\"english\")\n",
    "for idx, stop_word in enumerate(stopwords):\n",
    "    del words[stop_word]\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "        \n",
    "def build_wordlist(df, min_occurrences=10, max_occurences=10000, stopwords=nltk.corpus.stopwords.words(\"english\"),whitelist=None):\n",
    "    wordlist = []\n",
    "    whitelist = whitelist if whitelist is None else whitelist\n",
    "    import os\n",
    "    if os.path.isfile(\"data\\\\wordlistNew.csv\"):\n",
    "        word_df = pd.read_csv(\"data\\\\wordlistNew.csv\")\n",
    "        word_df = word_df[word_df[\"occurrences\"] > min_occurrences]\n",
    "        wordlist = list(word_df.loc[:, \"word\"])\n",
    "        return wordlist\n",
    "\n",
    "    words = Counter()\n",
    "    for idx in df.index:\n",
    "        words.update(df.loc[idx,\"tweet_text\"])\n",
    "\n",
    "    for idx, stop_word in enumerate(stopwords):\n",
    "        del words[stop_word]\n",
    "\n",
    "    word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "    word_df.to_csv(\"data\\\\wordlistNew.csv\", index_label=\"idx\")\n",
    "    wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = build_wordlist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.read_csv(\"data\\\\wordlistNew.csv\")\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "x_words = list(words.loc[0:10,\"word\"])\n",
    "x_words.reverse()\n",
    "y_occ = list(words.loc[0:10,\"occurrences\"])\n",
    "y_occ.reverse()\n",
    "\n",
    "dist = [\n",
    "    go.Bar(\n",
    "        x=y_occ,\n",
    "        y=x_words,\n",
    "        orientation=\"h\"\n",
    ")]\n",
    "pyo.iplot({\"data\":dist, \"layout\":graph_objs.Layout(title=\"Top words in built wordlist\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_data_model(df,is_testing, wordlist):\n",
    "        label_column = []\n",
    "        if not is_testing:\n",
    "            label_column = [\"label\"]\n",
    "\n",
    "        columns = label_column + list(\n",
    "            map(lambda w: w + \"_bow\",wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in df.index:\n",
    "            current_row = []\n",
    "\n",
    "            if not is_testing:\n",
    "                # add label\n",
    "                current_label = df.loc[idx, \"loc\"]\n",
    "                labels.append(current_label)\n",
    "                current_row.append(current_label)\n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(df.loc[idx, \"tweet_text\"])\n",
    "            for _, word in enumerate(wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        data_model = pd.DataFrame(rows, columns=columns)\n",
    "        data_labels = pd.Series(labels)\n",
    "        return data_model, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = TwitterData_BagOfWords(data)\n",
    "bow, labels = build_data_model(df, False, wordlist)\n",
    "bowP, labelsP = build_data_model(dfPred, True, wordlist)\n",
    "bowP.head(5)\n",
    "bow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEXtra = pd.read_csv('trainExtraColumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEXtra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPredExtra = pd.read_csv('testExtraColumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPredExtra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyDf = pd.read_csv('NYWords', sep=\"\\n\",encoding='latin-1', header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caDf = pd.read_csv('caliWords', sep=\"\\n\",encoding='latin-1', header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaDf = pd.read_csv('geoWords', sep=\"\\n\",encoding='latin-1', header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyDf = nyDf[0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caDf = caDf[0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaDf = gaDf[0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWords = nyDf + caDf + gaDf\n",
    "allWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in allWords:\n",
    "    dfPred[\"is\"+word] = np.where(dfPred['tweet_text'].str.contains(word), 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def count_by_regex(regex, plain_text):\n",
    "#     return len(regex.findall(plain_text))\n",
    "\n",
    "\n",
    "# def addNewColumns():\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_by_lambda(expression, word_array):\n",
    "    return len(list(filter(expression, word_array)))\n",
    "def count_occurences(character, word_array):\n",
    "    counter = 0\n",
    "    for (j, word) in enumerate(word_array):\n",
    "        for char in word:\n",
    "            if char == character:\n",
    "                counter += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "def add_column(dfPred, column_name, column_content):\n",
    "    dfPred.loc[:, column_name] = pd.Series(column_content, index=dfPred.index)\n",
    "# add_column(df, 'splitted_text', map(lambda txt: txt.split(' '),\n",
    "#                df['tweet_text']))\n",
    "\n",
    "for word in allWords:\n",
    "    count = list(map(lambda txt: count_occurences(word, txt),\n",
    "                     dfPred['tweet_text']))\n",
    "\n",
    "    add_column(dfPred, 'is'+word, count)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row_value in dfPred['tweet_text'].iteritems():\n",
    "    tweets = dfPred['tweet_text'][i]\n",
    "#     print(tweets)\n",
    "    for word in allWords:\n",
    "#         print(word)\n",
    "#         print[]\n",
    "        if(any(i.lower() == word.lower() for i in tweets)):\n",
    "            print(\"inside if setting 1\")\n",
    "            dfPred['is'+word][i] = 1\n",
    "        \n",
    "    \n",
    "# df['tweet_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row_value in df['tweet_text'].iteritems():\n",
    "    tweets = df['tweet_text'][i]\n",
    "#     print(tweets)\n",
    "    for word in allWords:\n",
    "#         print(word)\n",
    "#         print[]\n",
    "        if(any(i.lower() == word.lower() for i in tweets)):\n",
    "            print(\"inside if setting 1\")\n",
    "            df['is'+word][i] = 1\n",
    "        \n",
    "    \n",
    "# df['tweet_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresdf = df.loc[:, 'isDeadass':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresdfPred = dfPred.loc[:, 'isDeadass':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labels)\n",
    "grouped = bow.groupby([\"label\"]).sum()\n",
    "# print(grouped)\n",
    "words_to_visualize = []\n",
    "sentiments = [\"NewYork\",\"Georgia\",\"California\"]\n",
    "#get the most 7 common words for every sentiment\n",
    "for sentiment in sentiments:\n",
    "    words = grouped.loc[sentiment,:]\n",
    "    words.sort_values(inplace=True,ascending=False)\n",
    "    for w in words.index[:7]:\n",
    "        if w not in words_to_visualize:\n",
    "            words_to_visualize.append(w)\n",
    "            \n",
    "            \n",
    "#visualize it\n",
    "plot_data = []\n",
    "for sentiment in sentiments:\n",
    "    plot_data.append(go.Bar(\n",
    "            x = [w.split(\"_\")[0] for w in words_to_visualize],\n",
    "            y = [grouped.loc[sentiment,w] for w in words_to_visualize],\n",
    "            name = sentiment\n",
    "    ))\n",
    "    \n",
    "pyo.iplot({\n",
    "        \"data\":plot_data,\n",
    "        \"layout\":go.Layout(title=\"Most common words across sentiments\")\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowFinal = pd.concat([bow.reset_index(drop=True),dfEXtra.reset_index(drop=True), featuresdf.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowFinal = bowFinal.drop('tweet_id', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowPredFinal = pd.concat([bowP.reset_index(drop=True),dfPredExtra.reset_index(drop=True), featuresdfPred.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowPredFinal = bowPredFinal.drop('tweet_id', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowFinal = bowFinal.drop('number_of_question', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowPredFinal = bowPredFinal.drop('number_of_question', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = [\"NewYork\",\"Georgia\",\"California\"]\n",
    "plots_data_ef = []\n",
    "for what in map(lambda o: \"number_of_\"+o,[\"exclamation\"]):\n",
    "# for what in map(lambda o: \"is\"+o,allWords):\n",
    "    ef_grouped = bowFinal[bowFinal[what]>=1].groupby([\"label\"]).count()\n",
    "    plots_data_ef.append({\"data\":[graph_objs.Bar(\n",
    "            x = sentiments,\n",
    "            y = [ef_grouped.loc[s,:][0] for s in sentiments],\n",
    "    )], \"title\":\"How feature \\\"\"+what+\"\\\" separates the tweets\"})\n",
    "    \n",
    "\n",
    "for plot_data_ef in plots_data_ef:\n",
    "    plotly.offline.iplot({\n",
    "            \"data\":plot_data_ef[\"data\"],\n",
    "            \"layout\":graph_objs.Layout(title=plot_data_ef[\"title\"])\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 5\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(X_train, y_train, X_test, y_test,bowP, classifier):\n",
    "    log(\"\")\n",
    "    log(\"===============================================\")\n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    log(\"Testing \" + classifier_name)\n",
    "    now = time()\n",
    "    list_of_labels = sorted(list(set(y_train)))\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "    log(\"Learing time {0}s\".format(time() - now))\n",
    "    now = time()\n",
    "    predictions = model.predict(X_test)\n",
    "    predictionsForTest = model.predict(bowP)\n",
    "    print(predictionsForTest)\n",
    "    log(\"Predicting time {0}s\".format(time() - now))\n",
    "\n",
    "    precision = precision_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    recall = recall_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    log(\"=================== Results ===================\")\n",
    "    log(\"NewYork     Georgia     California\")\n",
    "    log(\"F1       \" + str(f1))\n",
    "    log(\"Precision\" + str(precision))\n",
    "    log(\"Recall   \" + str(recall))\n",
    "    log(\"Accuracy \" + str(accuracy))\n",
    "    log(\"===============================================\")\n",
    "\n",
    "    return precision, recall, accuracy, f1, predictionsForTest\n",
    "\n",
    "def log(x):\n",
    "    #can be used to write to log file\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from xgboost import XGBClassifier as LogisticRegression\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow.iloc[:, 1:], bow.iloc[:, 0],train_size=0.65,\n",
    "                                                   random_state=seed)\n",
    "print('X_train')\n",
    "print(X_train)\n",
    "print('X_test')\n",
    "print(X_test)\n",
    "print('y_train')\n",
    "\n",
    "print(y_train)\n",
    "list_of_labels = sorted(list(set(y_train)))\n",
    "print(list_of_labels)\n",
    "print('y_test')\n",
    "\n",
    "print(y_test)\n",
    "\n",
    "precision, recall, accuracy, f1, predictionsForTest = test_classifier(X_train, y_train, X_test, y_test, bowP, KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(bowFinal.iloc[:, 1:], bowFinal.iloc[:, 0],train_size=0.65,\n",
    "                                                   random_state=seed)\n",
    "print('X_train')\n",
    "print(X_train)\n",
    "print('X_test')\n",
    "print(X_test)\n",
    "print('y_train')\n",
    "\n",
    "print(y_train)\n",
    "list_of_labels = sorted(list(set(y_train)))\n",
    "print(list_of_labels)\n",
    "print('y_test')\n",
    "\n",
    "print(y_test)\n",
    "\n",
    "precision, recall, accuracy, f1, predictionsForTest = test_classifier(X_train, y_train, X_test, y_test, bowPredFinal, SGDClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(bowFinal.iloc[:, 1:], bowFinal.iloc[:, 0],train_size=0.65,\n",
    "                                                   random_state=seed)\n",
    "print('X_train')\n",
    "print(X_train)\n",
    "print('X_test')\n",
    "print(X_test)\n",
    "print('y_train')\n",
    "\n",
    "print(y_train)\n",
    "list_of_labels = sorted(list(set(y_train)))\n",
    "print(list_of_labels)\n",
    "print('y_test')\n",
    "\n",
    "print(y_test)\n",
    "\n",
    "precision, recall, accuracy, f1, predictionsForTest = test_classifier(X_train, y_train, X_test, y_test, bowPredFinal, BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in predictionsForTest:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(r'outputPredXGBNoHashNoQWords.txt', predictionsForTest, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"test_tweets_copy.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [line.rstrip('\\n') for line in open('test_tweets_copy.txt', encoding = \"ISO-8859-1\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lines)):\n",
    "    lines[i] = lines[i][:-1]+predictionsForTest[i]\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesDf = pd.DataFrame(lines) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesDf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesDf.columns = [\"row\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_linesDf = linesDf.pop('row').str.split(',')\n",
    "linesDf['tweet-id'],linesDf['class'] = tmp_linesDf.str[0],tmp_linesDf.str[-1]\n",
    "\n",
    "print(linesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesDf.to_csv('kaggleOutputSDGNoHashNoQWords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
